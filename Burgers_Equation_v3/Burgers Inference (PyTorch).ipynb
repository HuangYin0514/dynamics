{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
    "\n",
    "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
    "\n",
    "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
    "\n",
    "@article{raissi2017physicsI,\n",
    "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10561},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@article{raissi2017physicsII,\n",
    "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10566},\n",
    "  year={2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Run Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /kaggle/working/\n",
    "# !rm -rf /kaggle/working/dynamics\n",
    "# !git clone https://github.com/HuangYin0514/dynamics.git\n",
    "# %cd /kaggle/working/dynamics/\n",
    "\n",
    "# !pip install pyDOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "\n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(\n",
    "                (\"layer_%d\" % i, torch.nn.Linear(layers[i], layers[i + 1]))\n",
    "            )\n",
    "            layer_list.append((\"activation_%d\" % i, self.activation()))\n",
    "\n",
    "        layer_list.append(\n",
    "            (\"layer_%d\" % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "\n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n",
    "\n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "        # data\n",
    "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.nu = nu\n",
    "\n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "        # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(),\n",
    "            lr=1.0,\n",
    "            max_iter=50000,\n",
    "            max_eval=50000,\n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5,\n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\",  # can be \"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"using cpu for optim...\")\n",
    "            self.optimizer = torch.optim.LBFGS(\n",
    "                self.dnn.parameters(),\n",
    "                lr=1.0,\n",
    "                max_iter=5,\n",
    "                max_eval=5,\n",
    "                history_size=50,\n",
    "                tolerance_grad=1e-5,\n",
    "                tolerance_change=1.0 * np.finfo(float).eps,\n",
    "                line_search_fn=\"strong_wolfe\",  # can be \"strong_wolfe\"\n",
    "            )\n",
    "\n",
    "        self.iter = 0\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        \"\"\"The pytorch autograd version of calculating residual\"\"\"\n",
    "        u = self.net_u(x, t)\n",
    "\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x,\n",
    "            x,\n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        f = u_t + u * u_x - self.nu * u_xx\n",
    "        return f\n",
    "\n",
    "    def loss_func(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        u_pred = self.net_u(self.x_u, self.t_u)\n",
    "        f_pred = self.net_f(self.x_f, self.t_f)\n",
    "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
    "        loss_f = torch.mean(f_pred ** 2)\n",
    "\n",
    "        loss = loss_u + loss_f\n",
    "\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                \"Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e\"\n",
    "                % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        self.dnn.train()\n",
    "\n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "noise = 0.0\n",
    "\n",
    "N_u = 100\n",
    "N_f = 10000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat(\"data/burgers_shock.mat\")\n",
    "\n",
    "t = data[\"t\"].flatten()[:, None]\n",
    "x = data[\"x\"].flatten()[:, None]\n",
    "Exact = np.real(data[\"usol\"]).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "xx1 = np.hstack((X[0:1, :].T, T[0:1, :].T))\n",
    "uu1 = Exact[0:1, :].T\n",
    "xx2 = np.hstack((X[:, 0:1], T[:, 0:1]))\n",
    "uu2 = Exact[:, 0:1]\n",
    "xx3 = np.hstack((X[:, -1:], T[:, -1:]))\n",
    "uu3 = Exact[:, -1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu for optim...\n"
     ]
    }
   ],
   "source": [
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.57 s, sys: 2.09 s, total: 10.7 s\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "               \n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error u: 9.887459e-01\n"
     ]
    }
   ],
   "source": [
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "print(\"Error u: %e\" % (error_u))\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method=\"cubic\")\n",
    "Error = np.abs(Exact - U_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
